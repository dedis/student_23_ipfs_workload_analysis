# Part 1 - Public Gateway Dataset Analysis

The scripts are arranged in this README.md in the same order as we executed them for the report:

- [convert_nginx_to_csv.py](convert_nginx_to_csv.py): Converts the `nginx` web server log file to a .csv file for further processing. Also filters out entries that do not contain a CID or are nonsensical in some other sense.

- [split_dataset.py](split_dataset.py): Splits the dataset into two parts, requests generated by third-party websites and requests generated directly. Also creates a pie chart showing the distribution between these two types.

Then, we only continued with the part of the dataset that consists of requests generated by third-party websites:

- [extract_URL.py](extract_URL.py): Extracts all URLs from the dataset and stores them in a .csv file, sorted by their occurence (most common websites first).
We then performed a manual classification on the top 100 websites as described in the report.

- [create_URL_pie_chart_with_types.py](create_URL_pie_chart_with_types.py): Takes the extracted URLs together with the manual classification and creates a pie chart as seen in the report.

- [sort_referrals.py](sort_referrals.py): Sorts the part dataset by user agent and http referrer.

- [isolate_domains.py](isolate_domains.py): Isolates the top 12 domains and puts them in a new folder.

For the report, we focused on the top website, `nunuyy.top`.
We analyzed some of the other top websites for the midterm presentation.

- [isolated_domains/get_basic_info.py](isolated_domains/get_basic_info.py): Prints out some basic information about the top websites in the folder. Useful to get a quick high level understanding of the data we are dealing with.

- [concatenate_continuous_requests.py](concatenate_continuous_requests.py): Concatenates requests "streams" that are related in the sense that they are from the same user agent visiting the exact same URL. The best example for this is a user watching a movie on some streaming website: As the user is watching the movie, many related requests will be generated over the span of seconds, minutes or even hours. By concatenating these requests, we put the important information in a single line, i.e. the duration of this request "stream", the starting time, the ending time, the cache hit ratio, the success ratio, and so on. Check the code to better understand what's happening. :-)

- [analyze_continuous_requests.py](analyze_continuous_requests.py): Creates several plots based on the concatenated requests, most useful to analyze e.g.:
    - the number of requests per stream
    - duration of such a stream (think duration that a user is watching a movie)
    - Success ratio of a stream
    - Cache hit ratio of a stream
